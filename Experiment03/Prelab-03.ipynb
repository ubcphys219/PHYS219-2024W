{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c58429c0-fe79-43b1-8c2d-694d392bd362",
   "metadata": {},
   "source": [
    "# Prelab 03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255fc3c3-2ff0-44c6-b75d-64b7cd4e4279",
   "metadata": {},
   "source": [
    "## Fitting damped harmonic oscillator transients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226e23fe-ea61-4536-add6-a8991e5418ba",
   "metadata": {},
   "source": [
    "The code below will generate data to fit. This will allow us to test the fitting code and make sure we understand it's limitations and how the data interacts with it. You can change the parameters as well as the spacing of points. Play around with the frequency and spacing and see how this changes your perception of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e020b50a-a17f-44c6-a79f-48fca649f9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to calculate and plot the transient response of a damped, harmonic oscillator\n",
    "\n",
    "# import the  necessary libraries and rename them\n",
    "import numpy as np\n",
    "import array\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "# Define the Parameter Names, and give them numerical values\n",
    "\n",
    "param_names = [\"amplitude\", \"tau\", \"resonant-freq\", \"phase\"]\n",
    "input = (1, 0.002, 500, np.pi/2)\n",
    "\n",
    "# set the range and spacing of points in the generated data\n",
    "start=0.0\n",
    "end=0.02\n",
    "spacing=0.0001\n",
    "\n",
    "uncertainty=0.01 #set an uncertainty in V\n",
    "\n",
    "# Define the Function for the Harmonic Oscillator Transient\n",
    "\n",
    "def fit_function(x, amplitude, tau, resonantf, phase):\n",
    "    return amplitude * np.exp(-x/tau) * np.cos(2.0*np.pi * resonantf * x + phase)\n",
    "\n",
    "\n",
    "# Define a set of x values that will be used for the calculation\n",
    "# Note that in your fitting code, x is defined differently \n",
    "#  - do not change that part of the fitting code when you get there.\n",
    "\n",
    "x = np.arange(start, end, spacing)\n",
    "y_err = uncertainty\n",
    "noise = (uncertainty)*np.random.randn(len(x)) # generates normally distributed scatter \n",
    "\n",
    "y = fit_function(x, *input)+noise\n",
    "\n",
    "\n",
    "plt.errorbar(x,y,yerr=y_err,marker='.',linestyle=\"\")\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"Voltage\")\n",
    "plt.title(\"Damped Oscillator\")\n",
    "\n",
    "# save and plot image \n",
    "plt.savefig(\"DampedOscillator1.jpeg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fefc25-9786-4a65-b8e8-a80689a79485",
   "metadata": {},
   "source": [
    "Now try fitting the data you've just generated. Since you're not reading a file in this time, the code is already modified to directly read the arrays above, but you will need to set up the inputs to use the appropriate function. Try using the same \"guesses\" as you input as the parameters, as well as parameters that are very different to see how the routine responds. Start by generating a dense grid of data, where by eye you can clearly see the oscillations, then try sparse data. If your guesses are exact, the fit will likely still return similar values, but if you change the guesses the fit will find a new local minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a949a83d-5d58-4fd4-a9ea-b0917b1010df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load python packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "###############################################################################\n",
    "# DEFINED FITTING FUNCTIONS\n",
    "###############################################################################\n",
    "\n",
    "def sine_func(x, amplitude, freq, phase):\n",
    "    return amplitude * np.sin(2.0 * np.pi * freq * x + phase)\n",
    "\n",
    "def offset_sine_func(x, amplitude, freq, phase, offset):\n",
    "    return (amplitude * np.sin(2.0 * np.pi * freq * x + phase)) + offset\n",
    "\n",
    "def exponential_func(x, amplitude, tau, voffset):\n",
    "    return amplitude * np.exp(x/(-1.0*tau)) + voffset\n",
    "\n",
    "def ringdown_function(x, amplitude, tau, resonantf, phase):\n",
    "    return amplitude * np.exp(-x/tau) * np.cos(2.0*np.pi * resonantf * x + phase)\n",
    "\n",
    "def linear_func(x, slope, intercept):\n",
    "    return slope * x + intercept\n",
    "\n",
    "###############################################################################\n",
    "# LIST OF ALL INPUTS\n",
    "###############################################################################\n",
    "\n",
    "# Name of the data file\n",
    "#fname = \"sintest2_pack.csv\"\n",
    "\n",
    "# Names and units of data columns from fname\n",
    "x_name = \"Time\"\n",
    "x_units = \"s\"\n",
    "y_name = \"Voltage\"\n",
    "y_units = \"V\"\n",
    "\n",
    "# Modify to change the fitting function, parameter names and to set initial parameter guesses\n",
    "fit_function = sine_func\n",
    "param_names = (\"amplitude\", \"freq\", \"phase\")\n",
    "guesses = (1, 500, 0.0)\n",
    "\n",
    "# Flags for optional features\n",
    "show_covariance_matrix = False\n",
    "set_xy_boundaries = False\n",
    "lower_x = -0.01 # these values ignored if set_xy_boundaries = False\n",
    "upper_x = 0.01\n",
    "lower_y = -1\n",
    "upper_y = 1\n",
    "\n",
    "###############################################################################\n",
    "# LOAD DATA\n",
    "###############################################################################\n",
    "\n",
    "# load the file fname and skip the first 'skiprows' rows\n",
    "#data = np.loadtxt(fname, delimiter=\",\", comments=\"#\", usecols=(0, 1, 2, 3), skiprows=2)\n",
    "\n",
    "# Assign the data file columns to variables for later use\n",
    "#x = data[:, 0]\n",
    "#y = data[:, 2]\n",
    "#y_sigma = data[:, 3]\n",
    "y_sigma=y_err\n",
    "\n",
    "###############################################################################\n",
    "# INITIAL PLOT OF THE DATA\n",
    "###############################################################################\n",
    "\n",
    "# Define 500 points spanning the range of the x-data; for plotting smooth curves\n",
    "xtheory = np.linspace(min(x), max(x), 500)\n",
    "\n",
    "# Compare the guessed curve to the data for visual reference\n",
    "y_guess = fit_function(xtheory, *guesses)\n",
    "plt.errorbar(x, y, yerr=y_sigma, marker=\".\", linestyle=\"\", label=\"Measured data\")\n",
    "plt.plot(\n",
    "    xtheory,\n",
    "    y_guess,\n",
    "    marker=\"\",\n",
    "    linestyle=\"-\",\n",
    "    linewidth=1,\n",
    "    color=\"g\",\n",
    "    label=\"Initial parameter guesses\",\n",
    ")\n",
    "plt.xlabel(f\"{x_name} [{x_units}]\")\n",
    "plt.ylabel(f\"{y_name} [{y_units}]\")\n",
    "plt.title(r\"Comparison between the data and the intial parameter guesses\")\n",
    "plt.legend(loc=\"best\", numpoints=1)\n",
    "plt.show()\n",
    "\n",
    "# calculate the value of the model at each of the x-values of the data set\n",
    "y_fit = fit_function(x, *guesses)\n",
    "\n",
    "# Residuals are the difference between the data and theory\n",
    "residual = y - y_fit\n",
    "\n",
    "# Plot the residuals\n",
    "plt.errorbar(x, residual, yerr=y_sigma, marker=\".\", linestyle=\"\", label=\"residuals\")\n",
    "plt.xlabel(f\"{x_name} [{x_units}]\")\n",
    "plt.ylabel(f\"Residual y-y_fit [{y_units}]\")\n",
    "plt.title(\"Residuals using initial parameter guesses\")\n",
    "plt.show()\n",
    "\n",
    "###############################################################################\n",
    "# PERFORM THE FIT AND PRINT RESULTS\n",
    "###############################################################################\n",
    "\n",
    "# Use curve_fit to perform the fit\n",
    "# fit_function: defined above to choose a specific fitting function \n",
    "# fit_params: holds the resulting fit parameters\n",
    "# fit_cov: the covariance matrix between all the parameters\n",
    "#          (used to extract fitting parameter uncertanties)\n",
    "# maxfev=10**5: maximum number of fitting procedure iterations before giving up\n",
    "# absolute_sigma=True: uncertainties are treated as absolute (not relative)\n",
    "fit_params, fit_cov = curve_fit(\n",
    "    fit_function, x, y, sigma=y_sigma, \n",
    "    p0=guesses,absolute_sigma=True, maxfev=10**5)\n",
    "\n",
    "# Define the function that calculates chi-squared\n",
    "def chi_square(fit_parameters, x, y, sigma):\n",
    "    dof = len(x) - len(fit_params)\n",
    "    return np.sum((y - fit_function(x, *fit_parameters)) ** 2 / sigma**2)/dof\n",
    "\n",
    "# Calculate and print reduced chi-squared\n",
    "chi2 = chi_square(fit_params, x, y, y_sigma)\n",
    "print(\"Chi-squared = \", chi2)\n",
    "\n",
    "# Calculate the uncertainties in the fit parameters\n",
    "fit_params_error = np.sqrt(np.diag(fit_cov))\n",
    "\n",
    "# Print the fit parameters with uncertianties\n",
    "print(\"\\nFit parameters:\")\n",
    "for i in range(len(fit_params)):\n",
    "    print(f\"   {param_names[i]} = {fit_params[i]:.3e} ± {fit_params_error[i]:.3e}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# (Optional) Print the covariance between all variables\n",
    "if show_covariance_matrix:\n",
    "    print(\"Covariance between fit parameters:\")\n",
    "    for i, fit_covariance in enumerate(fit_cov):\n",
    "        for j in range(i+1,len(fit_covariance)):\n",
    "            print(f\"   {param_names[i]} and {param_names[j]}: {fit_cov[i,j]:.3e}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# residual is the difference between the data and model\n",
    "x_fitfunc = np.linspace(min(x), max(x), len(x))\n",
    "y_fitfunc = fit_function(x_fitfunc, *fit_params)\n",
    "y_fit = fit_function(x, *fit_params)\n",
    "residual = y-y_fit\n",
    "\n",
    "###############################################################################\n",
    "# PRODUCE A MULTIPANEL PLOT, WITH SCATTER PLOT, RESIDUALS AND RESIDUALS HISTOGRAM\n",
    "###############################################################################\n",
    "\n",
    "# The size of the canvas\n",
    "fig = plt.figure(figsize=(7,15))\n",
    "\n",
    "# The scatter plot\n",
    "ax1 = fig.add_subplot(311)\n",
    "ax1.errorbar(x,y,yerr=y_sigma,marker='.',linestyle='',label=\"Measured data\")\n",
    "ax1.plot(x_fitfunc, y_fitfunc, marker=\"\", linestyle=\"-\", linewidth=2,color=\"r\", label=\"Fit\")\n",
    "ax1.set_xlabel(f\"{x_name} [{x_units}]\")\n",
    "ax1.set_ylabel(f\"{y_name} [{y_units}]\")\n",
    "ax1.set_title('Best Fit of Function to Data')\n",
    "\n",
    "# (Optional) set the x and y boundaries of your plot\n",
    "if set_xy_boundaries:\n",
    "    plt.xlim(lower_x,upper_x)\n",
    "    plt.ylim(lower_y,upper_y)\n",
    "# Show the legend. loc='best' places it where the date are least obstructed\n",
    "ax1.legend(loc='best',numpoints=1)\n",
    "\n",
    "# The residuals plot\n",
    "ax2 = fig.add_subplot(312)\n",
    "ax2.errorbar(x, residual, yerr=y_sigma,marker='.', linestyle='', label=\"Residual (y-y_fit)\")\n",
    "ax2.hlines(0,np.min(x),np.max(x),lw=2,alpha=0.8)\n",
    "ax2.set_xlabel(f\"{x_name} [{x_units}]\")\n",
    "ax2.set_ylabel(f\"y-y_fit [{y_units}]\")\n",
    "ax2.set_title('Residuals for the Best Fit')\n",
    "ax2.legend(loc='best',numpoints=1)\n",
    "\n",
    "# Histogram of the residuals\n",
    "ax3 = fig.add_subplot(313)\n",
    "hist,bins = np.histogram(residual,bins=30)\n",
    "ax3.bar(bins[:-1],hist,width=bins[1]-bins[0])\n",
    "ax3.set_ylim(0,1.2*np.max(hist))\n",
    "ax3.set_xlabel(f\"y-y_fit [{y_units}]\")\n",
    "ax3.set_ylabel('Number of occurences')\n",
    "ax3.set_title('Histogram of the Residuals')\n",
    "\n",
    "# Save a copy of the figure as a png \n",
    "plt.savefig('FittingResults.png')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea40ff05-a761-401d-8602-8564642f1644",
   "metadata": {},
   "source": [
    "### Advice to self for fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbee1fdb-cbfc-4e03-b1d7-3d494d2e97d3",
   "metadata": {},
   "source": [
    "Describe what you found as you changed the density of points and guesses. Give yourself some advice for how to set yourself up for success when taking data, and setting up the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141c3650-9239-43cd-a136-68ac6b81ffdd",
   "metadata": {},
   "source": [
    "## Analyzing noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104a7379-99b9-450c-b5f7-31aba788b279",
   "metadata": {},
   "source": [
    "Let's take a bit closer look at how we are treating noise. The \"Data Pack and Trim\" routine we've given you uses the ($1\\sigma$) standard deviation of your selected region to determine the noise. If your noise is gaussian, which it often is if you have fluctuating values, then this is very reasonable. However, you have probably seen that for some of your data in Experiments 1 or 2 sometimes you just see bit noise where the values are mostly constant, but sometimes jump to another mostly constant value. This comes from the digitization of the data, and depending on the acquisition parameters can overwhelm any gaussian noise present in the system.\n",
    "\n",
    "This bit noise works like any other digital noise: the \"true\" value lies somewhere between with equal probablility since we simply have no way to tell. Formally this can be treated as a \"boxcar\" distribution of possible values and should be given an uncertainty of $\\frac{1}{2}\\delta_{resolution}/\\sqrt{3}$ which is the standard deviation of this boxcar distribution with a width given by the resolution.\n",
    "\n",
    "In Experiment 3 your data will have a fairly large dynamic range: the initial response to a kick will be large, and you'll want as much of the ringdown as you can use. With a large signal, and some filtering or averaging within the oscillsocope you are very likely to see digitization/bit noise this week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18275d94-2043-4041-a1f9-3a27dc180ba2",
   "metadata": {},
   "source": [
    "### Digital noise example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22de299f-fab8-48ab-8090-baf2c423a376",
   "metadata": {},
   "source": [
    "Load the sample data file `LRC1CH1.csv` and isolate the first flat segment to examine the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e565d627-3e8a-4ce8-ac54-e41cfa52f93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the  necessary libraries and rename them\n",
    "import numpy as np\n",
    "import array\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "###############################################################################\n",
    "# LIST OF ALL INPUTS\n",
    "###############################################################################\n",
    "\n",
    "fname = 'sintest.csv' # file name containing your raw data\n",
    "\n",
    "title='My Packed MicSig Data' # give your graph a title\n",
    "output_name = 'sintest_packed.csv' # provide an output filename for the packed data\n",
    "\n",
    "# choose the data range you want to examine\n",
    "indexraw_min = 0\n",
    "indexraw_max = 5000\n",
    "\n",
    "# set y-scale to correspond to this range\n",
    "yregion_min=5.2\n",
    "yregion_max=4.8\n",
    "\n",
    "# select an uncertainty type\n",
    "# \"default\" uses the standard deviation of the selected flat region\n",
    "# if standard deviation is unreasonably small and/or shows only digitization noise with single bit flips,\n",
    "# set uncertainty type to \"digital\" to take the standard deviation of a boxcar defined by the max/min values\n",
    "# or, set uncertainty type to \"manual\" and set a value for manualsigma\n",
    "uncertainty = \"default\"\n",
    "manualsigma = 0\n",
    "\n",
    "npac=100 # define packing factor  npac\n",
    "\n",
    "# trim data set range before output to .csv file\n",
    "trim = 0 # default is 0 to save full dataset, set trim to 1 to trim the output data then give the range\n",
    "trim_min=50 # start of trimmed data by packed index\n",
    "trim_max=800 # end of trimmed data by packed index\n",
    "\n",
    "###############################################################################\n",
    "# READ IN DATA\n",
    "###############################################################################\n",
    "\n",
    "# read in data - the file is assumed to be in csv format (comma separated variables). \n",
    "#Files need to be specified with a full path OR they have to be saved in the same folder \n",
    "#as the script\n",
    "data = np.loadtxt(fname, delimiter=',', comments='#',usecols=(3,4),skiprows=1)\n",
    "#data = np.loadtxt(fname, delimiter=',',comments='#' )\n",
    "# access the data columns and assign variables xraw and yraw\n",
    "#generate  an array  xraw  which is the first  column  of  data.  Note the first column is \n",
    "#indexed as  zero.\n",
    "xraw = data[:,0]\n",
    "#generate  an array  yraw  which is the second  column  of  data  (index  1)\n",
    "yraw = data[:,1]\n",
    "#generate array containing index\n",
    "indexraw=np.arange(len(xraw))\n",
    "\n",
    "# plot data versus time and data versus index\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.scatter(xraw, yraw,marker='.')\n",
    "ax1.set_xlabel('Time (sec)')\n",
    "ax1.set_ylabel('Voltage (V)')\n",
    "ax1.set_title('My Raw MicSig Data')\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.scatter(indexraw,yraw,marker='.')\n",
    "ax2.set_xlabel('Index')\n",
    "ax2.set_ylabel('Voltage (V)')\n",
    "plt.show()\n",
    "\n",
    "###############################################################################\n",
    "# PLOT RESTRICTED INDEX RANGE OF FLAT DATA, CALCULATE UNCERTAINTY\n",
    "# plots a restricted index range where the data is flat\n",
    "# and calculates the standard deviation to assess the noise\n",
    "###############################################################################\n",
    "\n",
    "# change axis limits\n",
    "plt.xlim(indexraw_min,indexraw_max)\n",
    "plt.ylim(yregion_min,yregion_max)\n",
    "\n",
    "# plot the data versus index\n",
    "plt.scatter(indexraw,yraw,marker='.')\n",
    "\n",
    "# This next command displays the index plot. \n",
    "plt.show()\n",
    "\n",
    "#calculate and display the mean and standard deviation of the data that you have zoomed in on.\n",
    "y_ave = np.mean(yraw[indexraw_min:indexraw_max])\n",
    "y_std = np.std(yraw[indexraw_min:indexraw_max])\n",
    "print('mean = ',y_ave)\n",
    "print('standard deviation = ', y_std)\n",
    "\n",
    "# display a histogram of the data\n",
    "hist,bins = np.histogram(yraw[indexraw_min:indexraw_max],bins=20)\n",
    "plt.bar(bins[:-1],hist,width=bins[1]-bins[0])\n",
    "plt.ylim(0,1.2*np.max(hist))\n",
    "plt.xlabel('y_raw (Volts)')\n",
    "plt.ylabel('Number of occurences')\n",
    "plt.show()\n",
    "\n",
    "###############################################################################\n",
    "# PACK AND TRIM DATA\n",
    "###############################################################################\n",
    "\n",
    "#define a function  to pack the data\n",
    "def pack(A,p):\n",
    "  # A is an array, and p is the packing factor\n",
    "  B = np.zeros(len(A)//p)\n",
    "  i = 1\n",
    "  while i-1<len(B):\n",
    "    B[i-1] = np.mean(A[p*(i-1):p*i])\n",
    "    i += 1\n",
    "  return B\n",
    "# pack the data\n",
    "x=pack(xraw,npac)\n",
    "y=pack(yraw,npac)\n",
    "\n",
    "#create a vector that also has the integer index (index = 0,1,2 ... length-1)\n",
    "length=len(x)\n",
    "#print(length)\n",
    "index = np.arange(length)\n",
    "\n",
    "#create a vector that contains fixed uncertainty for x values (in this case set to zero\n",
    "sigmax = [0]*length\n",
    "#print(sigmax)\n",
    "\n",
    "#Create a vector that contains uncertainty of averaged y values. \n",
    "#sigmayraw is your estimate of the uncertainty in individual raw data points\n",
    "\n",
    "#Here it is taking that value from your previous statistics code \n",
    "#If the data sampled shows only a small amount of digitization noise, and the standard deviation is unreasonably small, \n",
    "#select \"digital\" above to take the stdev of a boxcar of the width of the digitization\n",
    "#If you think the standard deviation of your data is an underestimate of the uncertainty,\n",
    "#you can also enter a value by hand - just change the line that defines simayraw\n",
    "\n",
    "if uncertainty == \"digital\":\n",
    "    sigmayraw = 0.5 * (np.max(yraw[indexraw_min:indexraw_max])-np.min(yraw[indexraw_min:indexraw_max]))/np.sqrt(3)\n",
    "    print(\"digital uncertainty used:\",sigmayraw)\n",
    "elif uncertainty == \"manual\":\n",
    "    sigmayraw = manualsigma\n",
    "    print(\"manually specified uncertainty used:\",sigmayraw)\n",
    "else:\n",
    "    sigmayraw = y_std\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f005bc22-d85e-4853-84a2-f69dc914a6d6",
   "metadata": {},
   "source": [
    "Now, look at the data, and look at the standard deviation. Do you believe the uncertainty in this measurement corresponds to the standard deviation given here? Comment on the value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843a20ca-e971-4056-adb9-a409e02957e4",
   "metadata": {},
   "source": [
    "Another factor is the resolution of the oscilloscope on this particular vertical sensitivity. The data here rarely changes enough to even show any other values, but because it happens to here, we can determine the resolution from the bits jumped between. The Data Pack and Trim package has an option to analyze the \"digital\" noise (i.e. resolution limited) with the formula above. Try it now... change the `uncertainty` variable (on line 29) to the string `digital` and look at the output. Compare this to the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8b56b9-d27b-4a5d-94b7-87564113e9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the  necessary libraries and rename them\n",
    "import numpy as np\n",
    "import array\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "###############################################################################\n",
    "# LIST OF ALL INPUTS\n",
    "###############################################################################\n",
    "\n",
    "fname = 'sintest.csv' # file name containing your raw data\n",
    "\n",
    "title='My Packed MicSig Data' # give your graph a title\n",
    "output_name = 'sintest_packed.csv' # provide an output filename for the packed data\n",
    "\n",
    "# choose the data range you want to examine\n",
    "indexraw_min = 0\n",
    "indexraw_max = 5000\n",
    "\n",
    "# set y-scale to correspond to this range\n",
    "yregion_min=5.2\n",
    "yregion_max=4.8\n",
    "\n",
    "# select an uncertainty type\n",
    "# \"default\" uses the standard deviation of the selected flat region\n",
    "# if standard deviation is unreasonably small and/or shows only digitization noise with single bit flips,\n",
    "# set uncertainty type to \"digital\" to take the standard deviation of a boxcar defined by the max/min values\n",
    "# or, set uncertainty type to \"manual\" and set a value for manualsigma\n",
    "uncertainty = \"default\"\n",
    "manualsigma = 0\n",
    "\n",
    "npac=100 # define packing factor  npac\n",
    "\n",
    "# trim data set range before output to .csv file\n",
    "trim = 0 # default is 0 to save full dataset, set trim to 1 to trim the output data then give the range\n",
    "trim_min=50 # start of trimmed data by packed index\n",
    "trim_max=800 # end of trimmed data by packed index\n",
    "\n",
    "###############################################################################\n",
    "# READ IN DATA\n",
    "###############################################################################\n",
    "\n",
    "# read in data - the file is assumed to be in csv format (comma separated variables). \n",
    "#Files need to be specified with a full path OR they have to be saved in the same folder \n",
    "#as the script\n",
    "data = np.loadtxt(fname, delimiter=',', comments='#',usecols=(3,4),skiprows=1)\n",
    "#data = np.loadtxt(fname, delimiter=',',comments='#' )\n",
    "# access the data columns and assign variables xraw and yraw\n",
    "#generate  an array  xraw  which is the first  column  of  data.  Note the first column is \n",
    "#indexed as  zero.\n",
    "xraw = data[:,0]\n",
    "#generate  an array  yraw  which is the second  column  of  data  (index  1)\n",
    "yraw = data[:,1]\n",
    "#generate array containing index\n",
    "indexraw=np.arange(len(xraw))\n",
    "\n",
    "# plot data versus time and data versus index\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.scatter(xraw, yraw,marker='.')\n",
    "ax1.set_xlabel('Time (sec)')\n",
    "ax1.set_ylabel('Voltage (V)')\n",
    "ax1.set_title('My Raw MicSig Data')\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.scatter(indexraw,yraw,marker='.')\n",
    "ax2.set_xlabel('Index')\n",
    "ax2.set_ylabel('Voltage (V)')\n",
    "plt.show()\n",
    "\n",
    "###############################################################################\n",
    "# PLOT RESTRICTED INDEX RANGE OF FLAT DATA, CALCULATE UNCERTAINTY\n",
    "# plots a restricted index range where the data is flat\n",
    "# and calculates the standard deviation to assess the noise\n",
    "###############################################################################\n",
    "\n",
    "# change axis limits\n",
    "plt.xlim(indexraw_min,indexraw_max)\n",
    "plt.ylim(yregion_min,yregion_max)\n",
    "\n",
    "# plot the data versus index\n",
    "plt.scatter(indexraw,yraw,marker='.')\n",
    "\n",
    "# This next command displays the index plot. \n",
    "plt.show()\n",
    "\n",
    "#calculate and display the mean and standard deviation of the data that you have zoomed in on.\n",
    "y_ave = np.mean(yraw[indexraw_min:indexraw_max])\n",
    "y_std = np.std(yraw[indexraw_min:indexraw_max])\n",
    "print('mean = ',y_ave)\n",
    "print('standard deviation = ', y_std)\n",
    "\n",
    "# display a histogram of the data\n",
    "hist,bins = np.histogram(yraw[indexraw_min:indexraw_max],bins=20)\n",
    "plt.bar(bins[:-1],hist,width=bins[1]-bins[0])\n",
    "plt.ylim(0,1.2*np.max(hist))\n",
    "plt.xlabel('y_raw (Volts)')\n",
    "plt.ylabel('Number of occurences')\n",
    "plt.show()\n",
    "\n",
    "###############################################################################\n",
    "# PACK AND TRIM DATA\n",
    "###############################################################################\n",
    "\n",
    "#define a function  to pack the data\n",
    "def pack(A,p):\n",
    "  # A is an array, and p is the packing factor\n",
    "  B = np.zeros(len(A)//p)\n",
    "  i = 1\n",
    "  while i-1<len(B):\n",
    "    B[i-1] = np.mean(A[p*(i-1):p*i])\n",
    "    i += 1\n",
    "  return B\n",
    "# pack the data\n",
    "x=pack(xraw,npac)\n",
    "y=pack(yraw,npac)\n",
    "\n",
    "#create a vector that also has the integer index (index = 0,1,2 ... length-1)\n",
    "length=len(x)\n",
    "#print(length)\n",
    "index = np.arange(length)\n",
    "\n",
    "#create a vector that contains fixed uncertainty for x values (in this case set to zero\n",
    "sigmax = [0]*length\n",
    "#print(sigmax)\n",
    "\n",
    "#Create a vector that contains uncertainty of averaged y values. \n",
    "#sigmayraw is your estimate of the uncertainty in individual raw data points\n",
    "\n",
    "#Here it is taking that value from your previous statistics code \n",
    "#If the data sampled shows only a small amount of digitization noise, and the standard deviation is unreasonably small, \n",
    "#select \"digital\" above to take the stdev of a boxcar of the width of the digitization\n",
    "#If you think the standard deviation of your data is an underestimate of the uncertainty,\n",
    "#you can also enter a value by hand - just change the line that defines simayraw\n",
    "\n",
    "if uncertainty == \"digital\":\n",
    "    sigmayraw = (np.max(yraw[indexraw_min:indexraw_max])-np.min(yraw[indexraw_min:indexraw_max]))/np.sqrt(3)\n",
    "    print(\"digital uncertainty used:\",sigmayraw)\n",
    "elif uncertainty == \"manual\":\n",
    "    sigmayraw = manualsigma\n",
    "    print(\"manually specified uncertainty used:\",sigmayraw)\n",
    "else:\n",
    "    sigmayraw = y_std\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862c6e50-ce51-4be8-bc67-1fd72d11501c",
   "metadata": {},
   "source": [
    "Which uncertainty, the standard deviation or the digital resolution uncertainty seems more appropriate here? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a58fb45-da86-42ad-a244-8b968ab0c6c3",
   "metadata": {},
   "source": [
    "## Creating arrays and adding to them (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9d3e87-e44a-46a3-b954-72589d2c442a",
   "metadata": {},
   "source": [
    "In experiment 4, we'll use a limited spreadsheet-like interface (you may have seen it in 119), but if you want to store data that's being output by other parts of the code, you can create an array of that data on the fly. We'll use a numpy array because that will also allow us to do calculations and array operations.\n",
    "\n",
    "To create a numpy array we use the command: `np.array()`\n",
    "Only the first argument is required, and it needs to itself be an array.\n",
    "These are defined within square brackets with elements separated by commas: `[element1,element2,...]`\n",
    "A higher dimensional array is created by nesting the square brackets: `[[element11,element12],[element21,element22]]`\n",
    "and so on.\n",
    "\n",
    "Let's try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9afa90-a99e-4441-9feb-cd38c25dbe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([1,2,3]) # a 1D array of dimension 1x3\n",
    "print(\"A =\",A)\n",
    "\n",
    "B = np.array([[1],[2],[3]]) # a 2D array of dimension 3x1\n",
    "print(\"B =\",B)\n",
    "\n",
    "C = np.array([[11,12,13],[21,22,23]]) # a 2x3 2D array (a matrix\n",
    "print(\"C =\",C)\n",
    "\n",
    "Z = np.zeros((4,2)) # a 4x2 2D array of zeros, the dimensions need to be specified in brackets except in 1D\n",
    "print(\"Z =\",Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33f25fa-28ff-4339-b6ba-4985469ff0c8",
   "metadata": {},
   "source": [
    "If you want the first (ahem, the zeroth) element, you can specify that element and pull it out as a separate variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018800ae-eb8a-49da-9ced-f424676d35ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=A[0] # only need to specify the index\n",
    "print(\"a =\",a)\n",
    "\n",
    "b=B[0,0] # a vertical array needs to be treated like a 2D array, \n",
    "#and specify the index of the row, and column, otherwise it returns a vector of 1 element\n",
    "print(\"b =\",b)\n",
    "\n",
    "c=C[0,0]\n",
    "print(\"c =\",c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9250b91e-ce37-4556-a8e1-9a0a600cd52d",
   "metadata": {},
   "source": [
    "We can also replace a particular value by redefining the value for that index, and if you start with an array of zeros of the correct size, this is a good way to store data (but you need to know the final size of the array you expect):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cae612-0d5e-4842-80e4-67dc9b7b3420",
   "metadata": {},
   "outputs": [],
   "source": [
    "A[0]=0\n",
    "print(\"A =\",A)\n",
    "\n",
    "Z[0]=[1,0.1]\n",
    "Z[1]=[2,0.2]\n",
    "Z[2,1]=0.3\n",
    "Z[3,0]=4\n",
    "print(\"Z =\",Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f54be2f-12e8-4d68-8e91-db29161b7e72",
   "metadata": {},
   "source": [
    "Now, let's make this more interesting! What if we want to add values to an existing array? We can \"append\" them. To do this we can use the `np.append(array, values, axis)` function. This requires specifying the array we want to add to, the values to be added, and which axis to add them to, being careful that the dimensions match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8126327-1a62-4fa3-ac7a-bfb20321e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.append(A, [4], axis=0) # this will update the original array A with one more element\n",
    "print(\"A =\",A)\n",
    "\n",
    "C1 = np.append(C, [[31,32,33]], axis=0) # this will add another row to array C and store it as C1\n",
    "print(\"C1 =\",C1)\n",
    "\n",
    "C2 = np.append(C, [[14],[24]], axis=1) # this will add another column to array C and store it as C2\n",
    "print(\"C2 =\",C2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd10f02-346e-4661-8ce4-3c09744bb782",
   "metadata": {},
   "source": [
    "Now, run the cell above again! You will see that the array `A` continues to expand with the same additional element because we just redefined the variable! This can be useful for keeping a running tally of parameters, but you do have to be careful not to keep appending every time you run a cell (say to do something else)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d226c3-53e0-4cc4-8692-cc430e3aa9a1",
   "metadata": {},
   "source": [
    "### Make some arrays and modify them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb1f64e-f116-44e4-870e-5670d23efc12",
   "metadata": {},
   "source": [
    "Now you try... (note you will probably want to print out some intermediate steps so you can check that you're doing what you think you are doing!)\n",
    "* Create an array of zeros that is 2 x 5 and store it as a variable name\n",
    "* Modify the first row to have values `[3,3]` and the 2rd row to have `[2,1.5]`\n",
    "* Add `6.2` to all elements of the array\n",
    "* Mulitply all elements by `10`\n",
    "* Append another row containing values `[16,8.8]`\n",
    "* Print the value of the 2nd row, 2nd column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da796bf-f289-46b7-920b-2c63e02268b1",
   "metadata": {},
   "source": [
    "### Let's make an array of some of the parameters from our fitting..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074032-f0ec-4abe-83b7-fdf015398b2a",
   "metadata": {},
   "source": [
    "Run the code we used at the start a few more times with different point spacing and initial guesses. We'll store the point spacing, initial frequency, the output fit frequency, and the $\\chi^2$ as we go for each run. We'll start by pulling the last values you used above to set up the arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659b2d40-bfd9-4b9f-904b-bb2b9ad7dd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "realfreq=np.array([input[2]]) # the frequency is the 2nd element of the input array\n",
    "ptspacing=np.array([spacing])\n",
    "fitfreq=np.array([fit_params[2]]) # the frequency is again the 2nd element\n",
    "fitfreq_err=np.array([fit_params_error[2]]) # and we'll store the uncertainty as well\n",
    "chisquared=np.array([chi2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28888b3-4f08-4dd9-9dbf-995afb06709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Real frequency:\",realfreq)\n",
    "print(\"Point spacing:\",ptspacing)\n",
    "print(\"Fit frequency:\",fitfreq)\n",
    "print(\"Chi squared:\",chisquared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce276f7-1cab-43ba-8a16-0d406a90c3db",
   "metadata": {},
   "source": [
    "Now, change some parameters (point spacing, initial guesses) and we'll append the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a251047a-86f5-417f-96dc-346f00ed1c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to calculate and plot the transient response of a damped, harmonic oscillator\n",
    "\n",
    "# import the  necessary libraries and rename them\n",
    "import numpy as np\n",
    "import array\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "# Define the Parameter Names, and give them numerical values\n",
    "\n",
    "param_names = [\"amplitude\", \"tau\", \"resonant-freq\", \"phase\"]\n",
    "input = (1, 0.002, 500, np.pi/2)\n",
    "\n",
    "# set the range and spacing of points in the generated data\n",
    "start=0.0\n",
    "end=0.02\n",
    "spacing=0.0008\n",
    "\n",
    "uncertainty=0.01 #set an uncertainty in V\n",
    "\n",
    "# Define the Function for the Harmonic Oscillator Transient\n",
    "\n",
    "def fit_function(x, amplitude, tau, resonantf, phase):\n",
    "    return amplitude * np.exp(-x/tau) * np.cos(2.0*np.pi * resonantf * x + phase)\n",
    "\n",
    "\n",
    "# Define a set of x values that will be used for the calculation\n",
    "# Note that in your fitting code, x is defined differently \n",
    "#  - do not change that part of the fitting code when you get there.\n",
    "\n",
    "x = np.arange(start, end, spacing)\n",
    "y_err = uncertainty\n",
    "noise = (uncertainty)*np.random.randn(len(x)) # generates normally distributed scatter \n",
    "\n",
    "y = fit_function(x, *input)+noise\n",
    "\n",
    "\n",
    "plt.errorbar(x,y,yerr=y_err,marker='.',linestyle=\"\")\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"Voltage\")\n",
    "plt.title(\"Damped Oscillator\")\n",
    "\n",
    "# save and plot image \n",
    "plt.savefig(\"DampedOscillator1.jpeg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fe9c99-022a-49ce-a88d-43531bed001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load python packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "###############################################################################\n",
    "# DEFINED FITTING FUNCTIONS\n",
    "###############################################################################\n",
    "\n",
    "def sine_func(x, amplitude, freq, phase):\n",
    "    return amplitude * np.sin(2.0 * np.pi * freq * x + phase)\n",
    "\n",
    "def offset_sine_func(x, amplitude, freq, phase, offset):\n",
    "    return (amplitude * np.sin(2.0 * np.pi * freq * x + phase)) + offset\n",
    "\n",
    "def exponential_func(x, amplitude, tau, voffset):\n",
    "    return amplitude * np.exp(x/(-1.0*tau)) + voffset\n",
    "\n",
    "def ringdown_function(x, amplitude, tau, resonantf, phase):\n",
    "    return amplitude * np.exp(-x/tau) * np.cos(2.0*np.pi * resonantf * x + phase)\n",
    "\n",
    "def linear_func(x, slope, intercept):\n",
    "    return slope * x + intercept\n",
    "\n",
    "###############################################################################\n",
    "# LIST OF ALL INPUTS\n",
    "###############################################################################\n",
    "\n",
    "# Name of the data file\n",
    "#fname = \"sintest2_pack.csv\"\n",
    "\n",
    "# Names and units of data columns from fname\n",
    "x_name = \"Time\"\n",
    "x_units = \"s\"\n",
    "y_name = \"Voltage\"\n",
    "y_units = \"V\"\n",
    "\n",
    "# Modify to change the fitting function, parameter names and to set initial parameter guesses\n",
    "fit_function = ringdown_function\n",
    "param_names = (\"amplitude\", \"tau\", \"resonantf\", \"phase\")\n",
    "guesses = (1, 0.002, 200, 0.0)\n",
    "\n",
    "# Flags for optional features\n",
    "show_covariance_matrix = False\n",
    "set_xy_boundaries = False\n",
    "lower_x = -0.01 # these values ignored if set_xy_boundaries = False\n",
    "upper_x = 0.01\n",
    "lower_y = -1\n",
    "upper_y = 1\n",
    "\n",
    "###############################################################################\n",
    "# LOAD DATA\n",
    "###############################################################################\n",
    "\n",
    "# load the file fname and skip the first 'skiprows' rows\n",
    "#data = np.loadtxt(fname, delimiter=\",\", comments=\"#\", usecols=(0, 1, 2, 3), skiprows=2)\n",
    "\n",
    "# Assign the data file columns to variables for later use\n",
    "#x = data[:, 0]\n",
    "#y = data[:, 2]\n",
    "#y_sigma = data[:, 3]\n",
    "y_sigma=y_err\n",
    "\n",
    "###############################################################################\n",
    "# INITIAL PLOT OF THE DATA\n",
    "###############################################################################\n",
    "\n",
    "# Define 500 points spanning the range of the x-data; for plotting smooth curves\n",
    "xtheory = np.linspace(min(x), max(x), 500)\n",
    "\n",
    "# Compare the guessed curve to the data for visual reference\n",
    "y_guess = fit_function(xtheory, *guesses)\n",
    "plt.errorbar(x, y, yerr=y_sigma, marker=\".\", linestyle=\"\", label=\"Measured data\")\n",
    "plt.plot(\n",
    "    xtheory,\n",
    "    y_guess,\n",
    "    marker=\"\",\n",
    "    linestyle=\"-\",\n",
    "    linewidth=1,\n",
    "    color=\"g\",\n",
    "    label=\"Initial parameter guesses\",\n",
    ")\n",
    "plt.xlabel(f\"{x_name} [{x_units}]\")\n",
    "plt.ylabel(f\"{y_name} [{y_units}]\")\n",
    "plt.title(r\"Comparison between the data and the intial parameter guesses\")\n",
    "plt.legend(loc=\"best\", numpoints=1)\n",
    "plt.show()\n",
    "\n",
    "# calculate the value of the model at each of the x-values of the data set\n",
    "y_fit = fit_function(x, *guesses)\n",
    "\n",
    "# Residuals are the difference between the data and theory\n",
    "residual = y - y_fit\n",
    "\n",
    "# Plot the residuals\n",
    "plt.errorbar(x, residual, yerr=y_sigma, marker=\".\", linestyle=\"\", label=\"residuals\")\n",
    "plt.xlabel(f\"{x_name} [{x_units}]\")\n",
    "plt.ylabel(f\"Residual y-y_fit [{y_units}]\")\n",
    "plt.title(\"Residuals using initial parameter guesses\")\n",
    "plt.show()\n",
    "\n",
    "###############################################################################\n",
    "# PERFORM THE FIT AND PRINT RESULTS\n",
    "###############################################################################\n",
    "\n",
    "# Use curve_fit to perform the fit\n",
    "# fit_function: defined above to choose a specific fitting function \n",
    "# fit_params: holds the resulting fit parameters\n",
    "# fit_cov: the covariance matrix between all the parameters\n",
    "#          (used to extract fitting parameter uncertanties)\n",
    "# maxfev=10**5: maximum number of fitting procedure iterations before giving up\n",
    "# absolute_sigma=True: uncertainties are treated as absolute (not relative)\n",
    "fit_params, fit_cov = curve_fit(\n",
    "    fit_function, x, y, sigma=y_sigma, \n",
    "    p0=guesses,absolute_sigma=True, maxfev=10**5)\n",
    "\n",
    "# Define the function that calculates chi-squared\n",
    "def chi_square(fit_parameters, x, y, sigma):\n",
    "    dof = len(x) - len(fit_params)\n",
    "    return np.sum((y - fit_function(x, *fit_parameters)) ** 2 / sigma**2)/dof\n",
    "\n",
    "# Calculate and print reduced chi-squared\n",
    "chi2 = chi_square(fit_params, x, y, y_sigma)\n",
    "print(\"Chi-squared = \", chi2)\n",
    "\n",
    "# Calculate the uncertainties in the fit parameters\n",
    "fit_params_error = np.sqrt(np.diag(fit_cov))\n",
    "\n",
    "# Print the fit parameters with uncertianties\n",
    "print(\"\\nFit parameters:\")\n",
    "for i in range(len(fit_params)):\n",
    "    print(f\"   {param_names[i]} = {fit_params[i]:.3e} ± {fit_params_error[i]:.3e}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# (Optional) Print the covariance between all variables\n",
    "if show_covariance_matrix:\n",
    "    print(\"Covariance between fit parameters:\")\n",
    "    for i, fit_covariance in enumerate(fit_cov):\n",
    "        for j in range(i+1,len(fit_covariance)):\n",
    "            print(f\"   {param_names[i]} and {param_names[j]}: {fit_cov[i,j]:.3e}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# residual is the difference between the data and model\n",
    "x_fitfunc = np.linspace(min(x), max(x), len(x))\n",
    "y_fitfunc = fit_function(x_fitfunc, *fit_params)\n",
    "y_fit = fit_function(x, *fit_params)\n",
    "residual = y-y_fit\n",
    "\n",
    "###############################################################################\n",
    "# PRODUCE A MULTIPANEL PLOT, WITH SCATTER PLOT, RESIDUALS AND RESIDUALS HISTOGRAM\n",
    "###############################################################################\n",
    "\n",
    "# The size of the canvas\n",
    "fig = plt.figure(figsize=(7,15))\n",
    "\n",
    "# The scatter plot\n",
    "ax1 = fig.add_subplot(311)\n",
    "ax1.errorbar(x,y,yerr=y_sigma,marker='.',linestyle='',label=\"Measured data\")\n",
    "ax1.plot(x_fitfunc, y_fitfunc, marker=\"\", linestyle=\"-\", linewidth=2,color=\"r\", label=\"Fit\")\n",
    "ax1.set_xlabel(f\"{x_name} [{x_units}]\")\n",
    "ax1.set_ylabel(f\"{y_name} [{y_units}]\")\n",
    "ax1.set_title('Best Fit of Function to Data')\n",
    "\n",
    "# (Optional) set the x and y boundaries of your plot\n",
    "if set_xy_boundaries:\n",
    "    plt.xlim(lower_x,upper_x)\n",
    "    plt.ylim(lower_y,upper_y)\n",
    "# Show the legend. loc='best' places it where the date are least obstructed\n",
    "ax1.legend(loc='best',numpoints=1)\n",
    "\n",
    "# The residuals plot\n",
    "ax2 = fig.add_subplot(312)\n",
    "ax2.errorbar(x, residual, yerr=y_sigma,marker='.', linestyle='', label=\"Residual (y-y_fit)\")\n",
    "ax2.hlines(0,np.min(x),np.max(x),lw=2,alpha=0.8)\n",
    "ax2.set_xlabel(f\"{x_name} [{x_units}]\")\n",
    "ax2.set_ylabel(f\"y-y_fit [{y_units}]\")\n",
    "ax2.set_title('Residuals for the Best Fit')\n",
    "ax2.legend(loc='best',numpoints=1)\n",
    "\n",
    "# Histogram of the residuals\n",
    "ax3 = fig.add_subplot(313)\n",
    "hist,bins = np.histogram(residual,bins=30)\n",
    "ax3.bar(bins[:-1],hist,width=bins[1]-bins[0])\n",
    "ax3.set_ylim(0,1.2*np.max(hist))\n",
    "ax3.set_xlabel(f\"y-y_fit [{y_units}]\")\n",
    "ax3.set_ylabel('Number of occurences')\n",
    "ax3.set_title('Histogram of the Residuals')\n",
    "\n",
    "# Save a copy of the figure as a png \n",
    "plt.savefig('FittingResults.png')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d526b2-18b4-4168-9ab5-3ff4e6538cd6",
   "metadata": {},
   "source": [
    "Now let's append these new values to the array!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a780e700-fa47-4fd7-b321-957a4a114cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "realfreq=np.append(realfreq,[input[2]],axis=0)\n",
    "ptspacing=np.append(ptspacing,[spacing],axis=0)\n",
    "fitfreq=np.append(fitfreq,[fit_params[2]],axis=0)\n",
    "fitfreq_err=np.append(fitfreq_err,[fit_params_error[2]],axis=0)\n",
    "chisquared=np.append(chisquared,[chi2],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f7c0ba-2843-40f6-89db-9afa22909098",
   "metadata": {},
   "source": [
    "And print out our new array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43abc744-8bdc-4f1b-8f4c-29c9d3ba6668",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Real frequency:\",realfreq)\n",
    "print(\"Point spacing:\",ptspacing)\n",
    "print(\"Fit frequency:\",fitfreq)\n",
    "print(\"Fit frequency uncert:\",fitfreq_err)\n",
    "print(\"Chi squared:\",chisquared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73349e68-ab66-4ba5-a59c-f0383bf9d192",
   "metadata": {},
   "source": [
    "This is far from elegant, and when appending you need to take care not to accidentally add additional entries and scramble your data, but it's an easy way to start keeping track of parameter output as you go. Alternatively you can set up a suitably sized array of zeros using `np.zeros` from the start and slot in your entries, though this requires knowing in advance how large an array to make."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
